{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siva1202/Cgpa_calculator/blob/master/Untitled22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "import absl.logging\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "\n",
        "# Suppress warnings and set environment variables\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)\n",
        "\n",
        "# Ensure TensorFlow uses GPU if available\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    print(\"GPU available: \", physical_devices)\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting memory growth: {e}\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")"
      ],
      "metadata": {
        "id": "8clgKs2w5jce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "def load_data(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Data loaded successfully from {file_path}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_data(df, is_training=True, label_encoders=None):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Original features\n",
        "    numeric_cols = ['Transaction Amount', 'Quantity', 'Customer Age', 'Account Age Days', 'Transaction Hour']\n",
        "    categorical_cols = ['Payment Method', 'Product Category', 'Customer Location', 'Device Used']\n",
        "\n",
        "    # Handle missing values\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "    # Advanced Feature Engineering\n",
        "\n",
        "    # 1. Transaction Amount Features\n",
        "    df['Amount_Log'] = np.log1p(df['Transaction Amount'])\n",
        "    df['Amount_per_Quantity'] = df['Transaction Amount'] / (df['Quantity'] + 1)\n",
        "    df['Amount_zscore'] = stats.zscore(df['Transaction Amount'], nan_policy='omit')\n",
        "\n",
        "    # 2. Time-based Features\n",
        "    df['Hour_Bin'] = pd.cut(df['Transaction Hour'], bins=[-np.inf, 6, 12, 18, np.inf], labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
        "    df['Is_Weekend'] = (pd.to_datetime(df['Transaction Date']).dt.dayofweek >= 5).astype(int)\n",
        "    df['Day_of_Week'] = pd.to_datetime(df['Transaction Date']).dt.dayofweek\n",
        "\n",
        "    # 3. Customer Profile Features\n",
        "    df['Age_Category'] = pd.cut(df['Customer Age'], bins=[0, 25, 35, 50, 65, np.inf], labels=['Young', 'Young_Adult', 'Adult', 'Senior', 'Elder'])\n",
        "    df['Account_Age_Weeks'] = df['Account Age Days'] // 7\n",
        "    df['Is_New_Account'] = (df['Account Age Days'] <= 30).astype(int)\n",
        "\n",
        "    # 4. Transaction Pattern Features\n",
        "    df['Transaction_Size'] = pd.qcut(df['Transaction Amount'], q=5, labels=['Very_Small', 'Small', 'Medium', 'Large', 'Very_Large'], duplicates='drop')\n",
        "    df['Quantity_Log'] = np.log1p(df['Quantity'])\n",
        "\n",
        "    # 5. Location-Device Interaction\n",
        "    df['Location_Device'] = df['Customer Location'] + '_' + df['Device Used']\n",
        "\n",
        "    # 6. Risk Indicators\n",
        "    df['High_Amount_Flag'] = (df['Transaction Amount'] > df['Transaction Amount'].quantile(0.95)).astype(int)\n",
        "    df['High_Quantity_Flag'] = (df['Quantity'] > df['Quantity'].quantile(0.95)).astype(int)\n",
        "    df['Unusual_Hour_Flag'] = ((df['Transaction Hour'] < 6) | (df['Transaction Hour'] > 22)).astype(int)\n",
        "\n",
        "    # Handle categorical encoding\n",
        "    if is_training:\n",
        "        label_encoders = {}\n",
        "        for col in categorical_cols + ['Hour_Bin', 'Age_Category', 'Transaction_Size', 'Location_Device']:\n",
        "            if col in df.columns:\n",
        "                label_encoders[col] = LabelEncoder()\n",
        "                df[col] = label_encoders[col].fit_transform(df[col])\n",
        "        return df, label_encoders\n",
        "    else:\n",
        "        for col in categorical_cols + ['Hour_Bin', 'Age_Category', 'Transaction_Size', 'Location_Device']:\n",
        "            if col in df.columns and col in label_encoders:\n",
        "                df[col] = df[col].map(lambda x: 'Unknown' if x not in label_encoders[col].classes_ else x)\n",
        "                if 'Unknown' not in label_encoders[col].classes_:\n",
        "                    label_encoders[col].classes_ = np.append(label_encoders[col].classes_, 'Unknown')\n",
        "                df[col] = label_encoders[col].transform(df[col])\n",
        "        return df"
      ],
      "metadata": {
        "id": "wVCwHb825l_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle_imbalance\n",
        "def handle_imbalance(X, y):\n",
        "    print(\"Class distribution before SMOTE:\")\n",
        "    print(y.value_counts())\n",
        "\n",
        "    # Drop rows with NaN values in the target variable 'Is Fraudulent'\n",
        "    # Keep the original index using .index\n",
        "    nan_indices = y[y.isnull()].index\n",
        "\n",
        "    # Drop rows with NaN values from both X and y\n",
        "    y = y.dropna()\n",
        "    X = X.drop(index=nan_indices) # using .drop with index to drop the same rows from X\n",
        "\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "    print(\"\\nClass distribution after SMOTE:\")\n",
        "    print(y_resampled.value_counts())\n",
        "\n",
        "    return X_resampled, y_resampled"
      ],
      "metadata": {
        "id": "3GXVdy7B5ywC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base Model Trainer\n",
        "class BaseModelTrainer:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, solver='liblinear'),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=500, max_depth=30, n_jobs=-1),\n",
        "            'XGBoost': xgb.XGBClassifier(n_estimators=1000, max_depth=15, learning_rate=0.01, tree_method='hist', use_label_encoder=False)\n",
        "        }\n",
        "        self.best_models = {}\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        for model_name, model in self.models.items():\n",
        "            model.fit(X_train, y_train)\n",
        "            self.best_models[model_name] = model\n",
        "            gc.collect()\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        results = {}\n",
        "        for model_name, model in self.best_models.items():\n",
        "            y_pred = model.predict(X_val)\n",
        "            y_prob = model.predict_proba(X_val)[:, 1]\n",
        "            fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            accuracy = accuracy_score(y_val, y_pred)\n",
        "            precision = precision_score(y_val, y_pred)\n",
        "            recall = recall_score(y_val, y_pred)\n",
        "            f1 = f1_score(y_val, y_pred)\n",
        "            confusion = confusion_matrix(y_val, y_pred)\n",
        "            results[model_name] = {\n",
        "                'ROC AUC': roc_auc,\n",
        "                'Accuracy': accuracy,\n",
        "                'F1 Score': f1,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'y_prob': y_prob,\n",
        "                'Confusion Matrix': confusion\n",
        "            }\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "ovvEcc0f52GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model\n",
        "class NeuralNetworkModel:\n",
        "    def __init__(self, input_dim):\n",
        "        self.model = Sequential([\n",
        "            Input(shape=(input_dim,)),\n",
        "            Dense(1024, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(512, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(256, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val):\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        self.model.fit(X_train, y_train, epochs=200, batch_size=256, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        y_prob = self.model.predict(X_val).flatten()\n",
        "        y_pred = (y_prob > 0.5).astype(int)\n",
        "        fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision = precision_score(y_val, y_pred)\n",
        "        recall = recall_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred)\n",
        "        confusion = confusion_matrix(y_val, y_pred)\n",
        "        return {\n",
        "            'ROC AUC': roc_auc,\n",
        "            'Accuracy': accuracy,\n",
        "            'F1 Score': f1,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'y_prob': y_prob,\n",
        "            'Confusion Matrix': confusion\n",
        "            }\n",
        ""
      ],
      "metadata": {
        "id": "DwN1T3F757c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble Model\n",
        "class EnsembleModel:\n",
        "    def __init__(self, base_models, nn_model):\n",
        "        self.base_models = base_models\n",
        "        self.nn_model = nn_model\n",
        "        self.weights = None\n",
        "\n",
        "    def train(self, X_val, y_val):\n",
        "        base_preds = np.column_stack([model.predict_proba(X_val)[:, 1] for model in self.base_models.values()])\n",
        "        nn_preds = self.nn_model.model.predict(X_val).flatten()\n",
        "        all_preds = np.column_stack([base_preds, nn_preds])\n",
        "\n",
        "        # Use logistic regression to find optimal weights\n",
        "        meta_model = LogisticRegression()\n",
        "        meta_model.fit(all_preds, y_val)\n",
        "        self.weights = meta_model.coef_[0] / np.sum(meta_model.coef_[0])\n",
        "\n",
        "    def predict(self, X_val):\n",
        "        base_preds = np.column_stack([model.predict_proba(X_val)[:, 1] for model in self.base_models.values()])\n",
        "        nn_preds = self.nn_model.model.predict(X_val).flatten()\n",
        "        all_preds = np.column_stack([base_preds, nn_preds])\n",
        "        return np.dot(all_preds, self.weights)\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        y_prob = self.predict(X_val)\n",
        "        y_pred = (y_prob > 0.5).astype(int)\n",
        "        fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision = precision_score(y_val, y_pred)\n",
        "        recall = recall_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred)\n",
        "        confusion = confusion_matrix(y_val, y_pred)\n",
        "        return {\n",
        "            'ROC AUC': roc_auc,\n",
        "            'Accuracy': accuracy,\n",
        "            'F1 Score': f1,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'y_prob': y_prob,\n",
        "            'Confusion Matrix': confusion\n",
        "        }"
      ],
      "metadata": {
        "id": "hG2Se-Xb6C5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization function1\n",
        "def plot_auc_roc_curve(models_results, nn_results, ensemble_results, y_val):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for model_name, metrics in models_results.items():\n",
        "        fpr, tpr, _ = roc_curve(y_val, metrics['y_prob'])\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC: {metrics[\"ROC AUC\"]:.4f})')\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_val, nn_results['y_prob'])\n",
        "    plt.plot(fpr, tpr, label=f'Neural Network (AUC: {nn_results[\"ROC AUC\"]:.4f})', linestyle='-.')\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_val, ensemble_results['y_prob'])\n",
        "    plt.plot(fpr, tpr, label=f'Ensemble Model (AUC: {ensemble_results[\"ROC AUC\"]:.4f})', linestyle='--')\n",
        "\n",
        "    plt.title('ROC Curve Comparison')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "42f6UU2B6Gtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization function2\n",
        "def plot_metrics(models_results, nn_results, ensemble_results):\n",
        "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 16))\n",
        "\n",
        "    # Accuracy\n",
        "    ax1.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['Accuracy'],\n",
        "            models_results['Random Forest']['Accuracy'],\n",
        "            models_results['XGBoost']['Accuracy'],\n",
        "            nn_results['Accuracy'],\n",
        "            ensemble_results['Accuracy']])\n",
        "    ax1.set_title('Accuracy')\n",
        "\n",
        "    # F1 Score\n",
        "    ax2.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['F1 Score'],\n",
        "            models_results['Random Forest']['F1 Score'],\n",
        "            models_results['XGBoost']['F1 Score'],\n",
        "            nn_results['F1 Score'],\n",
        "            ensemble_results['F1 Score']])\n",
        "    ax2.set_title('F1 Score')\n",
        "\n",
        "    # Precision\n",
        "    ax3.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['Precision'],\n",
        "            models_results['Random Forest']['Precision'],\n",
        "            models_results['XGBoost']['Precision'],\n",
        "            nn_results['Precision'],\n",
        "            ensemble_results['Precision']])\n",
        "    ax3.set_title('Precision')\n",
        "\n",
        "    # Recall\n",
        "    ax4.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['Recall'],\n",
        "            models_results['Random Forest']['Recall'],\n",
        "            models_results['XGBoost']['Recall'],\n",
        "            nn_results['Recall'],\n",
        "            ensemble_results['Recall']])\n",
        "    ax4.set_title('Recall')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "def plot_confusion_matrices(models_results, nn_results, ensemble_results):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "    models_list = ['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble']\n",
        "    results_list = [models_results, models_results, models_results, nn_results, ensemble_results]\n",
        "\n",
        "    for i, (model_name, results) in enumerate(zip(models_list, results_list)):\n",
        "        ax = axes[i // 3, i % 3]\n",
        "        confusion = results[model_name]['Confusion Matrix'] if model_name in results else results['Confusion Matrix']\n",
        "        im = ax.imshow(confusion, cmap='Blues')\n",
        "        ax.set_title(f'Confusion Matrix - {model_name}')\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('Actual')\n",
        "\n",
        "        # Annotate the confusion matrix\n",
        "        for x in range(confusion.shape[0]):\n",
        "            for y in range(confusion.shape[1]):\n",
        "                ax.text(y, x, f'{confusion[x, y]}', ha='center', va='center', color='black')\n",
        "\n",
        "        fig.colorbar(im, ax=ax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "C_zQAjch6H7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run Pipeline\n",
        "def run_pipeline(train_df, test_df):\n",
        "    # Preprocess data\n",
        "    train_df, label_encoders = preprocess_data(train_df, is_training=True)\n",
        "    test_df = preprocess_data(test_df, is_training=False, label_encoders=label_encoders)\n",
        "\n",
        "    # Define feature columns and target including new engineered features\n",
        "    features = [\n",
        "        # Original features\n",
        "        'Transaction Amount', 'Quantity', 'Customer Age', 'Account Age Days', 'Transaction Hour',\n",
        "        'Payment Method', 'Product Category', 'Customer Location', 'Device Used',\n",
        "        # Amount-based features\n",
        "        'Amount_Log', 'Amount_per_Quantity', 'Amount_zscore',\n",
        "        # Time-based features\n",
        "        'Hour_Bin', 'Is_Weekend', 'Day_of_Week',\n",
        "        # Customer profile features\n",
        "        'Age_Category', 'Account_Age_Weeks', 'Is_New_Account',\n",
        "        # Transaction pattern features\n",
        "        'Transaction_Size', 'Quantity_Log',\n",
        "        # Location-Device interaction\n",
        "        'Location_Device',\n",
        "        # Risk indicators\n",
        "        'High_Amount_Flag', 'High_Quantity_Flag', 'Unusual_Hour_Flag'\n",
        "    ]\n",
        "\n",
        "    X = train_df[features]\n",
        "    y = train_df['Is Fraudulent']\n",
        "\n",
        "    # Handle class imbalance\n",
        "    X_resampled, y_resampled = handle_imbalance(X, y)\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    # Train base models\n",
        "    base_trainer = BaseModelTrainer()\n",
        "    base_trainer.train(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluate base models\n",
        "    base_results = base_trainer.evaluate(X_val_scaled, y_val)\n",
        "\n",
        "    # Train neural network model\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    nn_model = NeuralNetworkModel(input_dim=input_dim)\n",
        "    nn_model.train(X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "\n",
        "    # Evaluate neural network model\n",
        "    nn_results = nn_model.evaluate(X_val_scaled, y_val)\n",
        "\n",
        "    # Train and evaluate ensemble model\n",
        "    ensemble_model = EnsembleModel(base_models=base_trainer.best_models, nn_model=nn_model)\n",
        "    ensemble_model.train(X_val_scaled, y_val)\n",
        "    ensemble_results = ensemble_model.evaluate(X_val_scaled, y_val)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    rf_model = RandomForestClassifier(n_estimators=500, max_depth=30, n_jobs=-1)\n",
        "    rf_model.fit(X_train_scaled, y_train)\n",
        "    feature_importances = rf_model.feature_importances_\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(features, feature_importances)\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.title('Feature Importance')\n",
        "    plt.show()\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nBase Model Results:\")\n",
        "    for model_name, metrics in base_results.items():\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  - ROC AUC: {metrics['ROC AUC']:.4f}\")\n",
        "        print(f\"  - Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "        print(f\"  - F1 Score: {metrics['F1 Score']:.4f}\")\n",
        "        print(f\"  - Precision: {metrics['Precision']:.4f}\")\n",
        "        print(f\"  - Recall: {metrics['Recall']:.4f}\")\n",
        "        print(f\"  - Confusion Matrix:\\n{metrics['Confusion Matrix']}\\n\")\n",
        "\n",
        "    print(\"\\nNeural Network Results:\")\n",
        "    print(f\"  - ROC AUC: {nn_results['ROC AUC']:.4f}\")\n",
        "    print(f\"  - Accuracy: {nn_results['Accuracy']:.4f}\")\n",
        "    print(f\"  - F1 Score: {nn_results['F1 Score']:.4f}\")\n",
        "    print(f\"  - Precision: {nn_results['Precision']:.4f}\")\n",
        "    print(f\"  - Recall: {nn_results['Recall']:.4f}\")\n",
        "    print(f\"  - Confusion Matrix:\\n{nn_results['Confusion Matrix']}\\n\")\n",
        "\n",
        "    print(\"\\nEnsemble Model Results:\")\n",
        "    print(f\"  - ROC AUC: {ensemble_results['ROC AUC']:.4f}\")\n",
        "    print(f\"  - Accuracy: {ensemble_results['Accuracy']:.4f}\")\n",
        "    print(f\"  - F1 Score: {ensemble_results['F1 Score']:.4f}\")\n",
        "    print(f\"  - Precision: {ensemble_results['Precision']:.4f}\")\n",
        "    print(f\"  - Recall: {ensemble_results['Recall']:.4f}\")\n",
        "    print(f\"  - Confusion Matrix:\\n{ensemble_results['Confusion Matrix']}\\n\")\n",
        "\n",
        "    # Plot AUC-ROC comparison\n",
        "    plot_auc_roc_curve(base_results, nn_results, ensemble_results, y_val)\n",
        "\n",
        "    # Plot Metrics Comparison\n",
        "    plot_metrics(base_results, nn_results, ensemble_results)\n",
        "\n",
        "    # Plot Confusion Matrices\n",
        "    plot_confusion_matrices(base_results, nn_results, ensemble_results)\n",
        "\n",
        "# Path usage\n",
        "train_df = load_data('Book1.csv')\n",
        "test_df = load_data('data.csv')\n",
        "\n",
        "if train_df is not None and test_df is not None:\n",
        "    run_pipeline(train_df, test_df)"
      ],
      "metadata": {
        "id": "lT-w9ILa6UYW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1XoFx-GAS1ACt7c9Tjgrj_bDSzs8rdYPe",
      "authorship_tag": "ABX9TyM1sjoWKx9vJVeIIchXHKdQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}