{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siva1202/Cgpa_calculator/blob/master/Untitled22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k57RktP7LRY",
        "outputId": "19636c5f-44cf-49b3-e692-75cf7dbb63bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy matplotlib scikit-learn xgboost keras tensorflow imbalanced-learn absl-py scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNFKn-RcIFdb",
        "outputId": "042603c1-5b89-4767-9d55-54de4ba6d065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "import absl.logging\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "\n",
        "# Suppress warnings and set environment variables\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)\n",
        "\n",
        "# Ensure TensorFlow uses GPU if available\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    print(\"GPU available: \", physical_devices)\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting memory growth: {e}\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6g6QQ0SIXk0"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "def load_data(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Data loaded successfully from {file_path}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_data(df, is_training=True, label_encoders=None):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Original features\n",
        "    numeric_cols = ['Transaction Amount', 'Quantity', 'Customer Age', 'Account Age Days', 'Transaction Hour']\n",
        "    categorical_cols = ['Payment Method', 'Product Category', 'Customer Location', 'Device Used']\n",
        "\n",
        "    # Handle missing values\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "    # Advanced Feature Engineering\n",
        "\n",
        "    # 1. Transaction Amount Features\n",
        "    df['Amount_Log'] = np.log1p(df['Transaction Amount'])\n",
        "    df['Amount_per_Quantity'] = df['Transaction Amount'] / (df['Quantity'] + 1)\n",
        "    df['Amount_zscore'] = stats.zscore(df['Transaction Amount'], nan_policy='omit')\n",
        "\n",
        "    # 2. Time-based Features\n",
        "    df['Hour_Bin'] = pd.cut(df['Transaction Hour'], bins=[-np.inf, 6, 12, 18, np.inf], labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
        "    df['Is_Weekend'] = (pd.to_datetime(df['Transaction Date']).dt.dayofweek >= 5).astype(int)\n",
        "    df['Day_of_Week'] = pd.to_datetime(df['Transaction Date']).dt.dayofweek\n",
        "\n",
        "    # 3. Customer Profile Features\n",
        "    df['Age_Category'] = pd.cut(df['Customer Age'], bins=[0, 25, 35, 50, 65, np.inf], labels=['Young', 'Young_Adult', 'Adult', 'Senior', 'Elder'])\n",
        "    df['Account_Age_Weeks'] = df['Account Age Days'] // 7\n",
        "    df['Is_New_Account'] = (df['Account Age Days'] <= 30).astype(int)\n",
        "\n",
        "    # 4. Transaction Pattern Features\n",
        "    df['Transaction_Size'] = pd.qcut(df['Transaction Amount'], q=5, labels=['Very_Small', 'Small', 'Medium', 'Large', 'Very_Large'], duplicates='drop')\n",
        "    df['Quantity_Log'] = np.log1p(df['Quantity'])\n",
        "\n",
        "    # 5. Location-Device Interaction\n",
        "    df['Location_Device'] = df['Customer Location'] + '_' + df['Device Used']\n",
        "\n",
        "    # 6. Risk Indicators\n",
        "    df['High_Amount_Flag'] = (df['Transaction Amount'] > df['Transaction Amount'].quantile(0.95)).astype(int)\n",
        "    df['High_Quantity_Flag'] = (df['Quantity'] > df['Quantity'].quantile(0.95)).astype(int)\n",
        "    df['Unusual_Hour_Flag'] = ((df['Transaction Hour'] < 6) | (df['Transaction Hour'] > 22)).astype(int)\n",
        "\n",
        "    # Handle categorical encoding\n",
        "    if is_training:\n",
        "        label_encoders = {}\n",
        "        for col in categorical_cols + ['Hour_Bin', 'Age_Category', 'Transaction_Size', 'Location_Device']:\n",
        "            if col in df.columns:\n",
        "                label_encoders[col] = LabelEncoder()\n",
        "                df[col] = label_encoders[col].fit_transform(df[col])\n",
        "        return df, label_encoders\n",
        "    else:\n",
        "        for col in categorical_cols + ['Hour_Bin', 'Age_Category', 'Transaction_Size', 'Location_Device']:\n",
        "            if col in df.columns and col in label_encoders:\n",
        "                df[col] = df[col].map(lambda x: 'Unknown' if x not in label_encoders[col].classes_ else x)\n",
        "                if 'Unknown' not in label_encoders[col].classes_:\n",
        "                    label_encoders[col].classes_ = np.append(label_encoders[col].classes_, 'Unknown')\n",
        "                df[col] = label_encoders[col].transform(df[col])\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzDg_zB1I90c"
      },
      "outputs": [],
      "source": [
        "# Handle imbalance\n",
        "def handle_imbalance(X, y):\n",
        "    print(\"Class distribution before SMOTE:\")\n",
        "    print(y.value_counts())\n",
        "\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "    print(\"\\nClass distribution after SMOTE:\")\n",
        "    print(y_resampled.value_counts())\n",
        "\n",
        "    return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKRUKR3SJHDP"
      },
      "outputs": [],
      "source": [
        "# Base Model Trainer\n",
        "class BaseModelTrainer:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, solver='liblinear'),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=500, max_depth=30, n_jobs=-1),\n",
        "            'XGBoost': xgb.XGBClassifier(n_estimators=1000, max_depth=15, learning_rate=0.01, tree_method='hist', use_label_encoder=False)\n",
        "        }\n",
        "        self.best_models = {}\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        for model_name, model in self.models.items():\n",
        "            model.fit(X_train, y_train)\n",
        "            self.best_models[model_name] = model\n",
        "            gc.collect()\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        results = {}\n",
        "        for model_name, model in self.best_models.items():\n",
        "            y_pred = model.predict(X_val)\n",
        "            y_prob = model.predict_proba(X_val)[:, 1]\n",
        "            fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            accuracy = accuracy_score(y_val, y_pred)\n",
        "            precision = precision_score(y_val, y_pred)\n",
        "            recall = recall_score(y_val, y_pred)\n",
        "            f1 = f1_score(y_val, y_pred)\n",
        "            confusion = confusion_matrix(y_val, y_pred)\n",
        "            results[model_name] = {\n",
        "                'ROC AUC': roc_auc,\n",
        "                'Accuracy': accuracy,\n",
        "                'F1 Score': f1,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'y_prob': y_prob,\n",
        "                'Confusion Matrix': confusion\n",
        "            }\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhj8kX7RJUIz"
      },
      "outputs": [],
      "source": [
        "# Neural Network Model\n",
        "class NeuralNetworkModel:\n",
        "    def __init__(self, input_dim):\n",
        "        self.model = Sequential([\n",
        "            Input(shape=(input_dim,)),\n",
        "            Dense(1024, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(512, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(256, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val):\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        self.model.fit(X_train, y_train, epochs=200, batch_size=256, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        y_prob = self.model.predict(X_val).flatten()\n",
        "        y_pred = (y_prob > 0.5).astype(int)\n",
        "        fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision = precision_score(y_val, y_pred)\n",
        "        recall = recall_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred)\n",
        "        confusion = confusion_matrix(y_val, y_pred)\n",
        "        return {\n",
        "            'ROC AUC': roc_auc,\n",
        "            'Accuracy': accuracy,\n",
        "            'F1 Score': f1,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'y_prob': y_prob,\n",
        "            'Confusion Matrix': confusion\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5rAcUZ_JfKA"
      },
      "outputs": [],
      "source": [
        "# Ensemble Model\n",
        "class EnsembleModel:\n",
        "    def __init__(self, base_models, nn_model):\n",
        "        self.base_models = base_models\n",
        "        self.nn_model = nn_model\n",
        "        self.weights = None\n",
        "\n",
        "    def train(self, X_val, y_val):\n",
        "        base_preds = np.column_stack([model.predict_proba(X_val)[:, 1] for model in self.base_models.values()])\n",
        "        nn_preds = self.nn_model.model.predict(X_val).flatten()\n",
        "        all_preds = np.column_stack([base_preds, nn_preds])\n",
        "\n",
        "        # Use logistic regression to find optimal weights\n",
        "        meta_model = LogisticRegression()\n",
        "        meta_model.fit(all_preds, y_val)\n",
        "        self.weights = meta_model.coef_[0] / np.sum(meta_model.coef_[0])\n",
        "\n",
        "    def predict(self, X_val):\n",
        "        base_preds = np.column_stack([model.predict_proba(X_val)[:, 1] for model in self.base_models.values()])\n",
        "        nn_preds = self.nn_model.model.predict(X_val).flatten()\n",
        "        all_preds = np.column_stack([base_preds, nn_preds])\n",
        "        return np.dot(all_preds, self.weights)\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        y_prob = self.predict(X_val)\n",
        "        y_pred = (y_prob > 0.5).astype(int)\n",
        "        fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision = precision_score(y_val, y_pred)\n",
        "        recall = recall_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred)\n",
        "        confusion = confusion_matrix(y_val, y_pred)\n",
        "        return {\n",
        "            'ROC AUC': roc_auc,\n",
        "            'Accuracy': accuracy,\n",
        "            'F1 Score': f1,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'y_prob': y_prob,\n",
        "            'Confusion Matrix': confusion\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByxVwMHTJyO8"
      },
      "outputs": [],
      "source": [
        "# Visualization function1\n",
        "def plot_auc_roc_curve(models_results, nn_results, ensemble_results, y_val):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for model_name, metrics in models_results.items():\n",
        "        fpr, tpr, _ = roc_curve(y_val, metrics['y_prob'])\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC: {metrics[\"ROC AUC\"]:.4f})')\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_val, nn_results['y_prob'])\n",
        "    plt.plot(fpr, tpr, label=f'Neural Network (AUC: {nn_results[\"ROC AUC\"]:.4f})', linestyle='-.')\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_val, ensemble_results['y_prob'])\n",
        "    plt.plot(fpr, tpr, label=f'Ensemble Model (AUC: {ensemble_results[\"ROC AUC\"]:.4f})', linestyle='--')\n",
        "\n",
        "    plt.title('ROC Curve Comparison')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKnxS5zFJ9Ko"
      },
      "outputs": [],
      "source": [
        "# Visualization function2\n",
        "def plot_metrics(models_results, nn_results, ensemble_results):\n",
        "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 16))\n",
        "\n",
        "    # Accuracy\n",
        "    ax1.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['Accuracy'],\n",
        "            models_results['Random Forest']['Accuracy'],\n",
        "            models_results['XGBoost']['Accuracy'],\n",
        "            nn_results['Accuracy'],\n",
        "            ensemble_results['Accuracy']])\n",
        "    ax1.set_title('Accuracy')\n",
        "\n",
        "    # F1 Score\n",
        "    ax2.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['F1 Score'],\n",
        "            models_results['Random Forest']['F1 Score'],\n",
        "            models_results['XGBoost']['F1 Score'],\n",
        "            nn_results['F1 Score'],\n",
        "            ensemble_results['F1 Score']])\n",
        "    ax2.set_title('F1 Score')\n",
        "\n",
        "    # Precision\n",
        "    ax3.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['Precision'],\n",
        "            models_results['Random Forest']['Precision'],\n",
        "            models_results['XGBoost']['Precision'],\n",
        "            nn_results['Precision'],\n",
        "            ensemble_results['Precision']])\n",
        "    ax3.set_title('Precision')\n",
        "\n",
        "    # Recall\n",
        "    ax4.bar(['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble'],\n",
        "           [models_results['Logistic Regression']['Recall'],\n",
        "            models_results['Random Forest']['Recall'],\n",
        "            models_results['XGBoost']['Recall'],\n",
        "            nn_results['Recall'],\n",
        "            ensemble_results['Recall']])\n",
        "    ax4.set_title('Recall')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "def plot_confusion_matrices(models_results, nn_results, ensemble_results):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "    models_list = ['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Ensemble']\n",
        "    results_list = [models_results, models_results, models_results, nn_results, ensemble_results]\n",
        "\n",
        "    for i, (model_name, results) in enumerate(zip(models_list, results_list)):\n",
        "        ax = axes[i // 3, i % 3]\n",
        "        confusion = results[model_name]['Confusion Matrix'] if model_name in results else results['Confusion Matrix']\n",
        "        im = ax.imshow(confusion, cmap='Blues')\n",
        "        ax.set_title(f'Confusion Matrix - {model_name}')\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('Actual')\n",
        "\n",
        "        # Annotate the confusion matrix\n",
        "        for x in range(confusion.shape[0]):\n",
        "            for y in range(confusion.shape[1]):\n",
        "                ax.text(y, x, f'{confusion[x, y]}', ha='center', va='center', color='black')\n",
        "\n",
        "        fig.colorbar(im, ax=ax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "6EdKbpnUKLrq",
        "outputId": "3338dabe-1069-44e0-96c9-87cf8f17eca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully from Book1.csv\n",
            "Data loaded successfully from data.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-f92eb4a3b6fa>:36: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  df['Is_Weekend'] = (pd.to_datetime(df['Transaction Date']).dt.dayofweek >= 5).astype(int)\n",
            "<ipython-input-11-f92eb4a3b6fa>:37: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  df['Day_of_Week'] = pd.to_datetime(df['Transaction Date']).dt.dayofweek\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE:\n",
            "Is Fraudulent\n",
            "0.0    52940\n",
            "1.0     2746\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py:392: RuntimeWarning: invalid value encountered in cast\n",
            "  return x.astype(dtype, copy=copy, casting=casting)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4d788c3a1f44>\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-4d788c3a1f44>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Handle class imbalance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_imbalance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Split the data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-701d8118ea3b>\u001b[0m in \u001b[0;36mhandle_imbalance\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nClass distribution after SMOTE:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mTarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     if y_type not in [\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name, raise_unknown)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ],
      "source": [
        "# Run Pipeline\n",
        "def run_pipeline(train_df, test_df):\n",
        "    # Preprocess data\n",
        "    train_df, label_encoders = preprocess_data(train_df, is_training=True)\n",
        "    test_df = preprocess_data(test_df, is_training=False, label_encoders=label_encoders)\n",
        "\n",
        "    # Define feature columns and target including new engineered features\n",
        "    features = [\n",
        "        # Original features\n",
        "        'Transaction Amount', 'Quantity', 'Customer Age', 'Account Age Days', 'Transaction Hour',\n",
        "        'Payment Method', 'Product Category', 'Customer Location', 'Device Used',\n",
        "        # Amount-based features\n",
        "        'Amount_Log', 'Amount_per_Quantity', 'Amount_zscore',\n",
        "        # Time-based features\n",
        "        'Hour_Bin', 'Is_Weekend', 'Day_of_Week',\n",
        "        # Customer profile features\n",
        "        'Age_Category', 'Account_Age_Weeks', 'Is_New_Account',\n",
        "        # Transaction pattern features\n",
        "        'Transaction_Size', 'Quantity_Log',\n",
        "        # Location-Device interaction\n",
        "        'Location_Device',\n",
        "        # Risk indicators\n",
        "        'High_Amount_Flag', 'High_Quantity_Flag', 'Unusual_Hour_Flag'\n",
        "    ]\n",
        "\n",
        "    X = train_df[features]\n",
        "    y = train_df['Is Fraudulent']\n",
        "\n",
        "    # Handle class imbalance\n",
        "    X_resampled, y_resampled = handle_imbalance(X, y)\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    # Train base models\n",
        "    base_trainer = BaseModelTrainer()\n",
        "    base_trainer.train(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluate base models\n",
        "    base_results = base_trainer.evaluate(X_val_scaled, y_val)\n",
        "\n",
        "    # Train neural network model\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    nn_model = NeuralNetworkModel(input_dim=input_dim)\n",
        "    nn_model.train(X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "\n",
        "    # Evaluate neural network model\n",
        "    nn_results = nn_model.evaluate(X_val_scaled, y_val)\n",
        "\n",
        "    # Train and evaluate ensemble model\n",
        "    ensemble_model = EnsembleModel(base_models=base_trainer.best_models, nn_model=nn_model)\n",
        "    ensemble_model.train(X_val_scaled, y_val)\n",
        "    ensemble_results = ensemble_model.evaluate(X_val_scaled, y_val)\n",
        "\n",
        "    # Calculate feature importance\n",
        "    rf_model = RandomForestClassifier(n_estimators=500, max_depth=30, n_jobs=-1)\n",
        "    rf_model.fit(X_train_scaled, y_train)\n",
        "    feature_importances = rf_model.feature_importances_\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(features, feature_importances)\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.title('Feature Importance')\n",
        "    plt.show()\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nBase Model Results:\")\n",
        "    for model_name, metrics in base_results.items():\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  - ROC AUC: {metrics['ROC AUC']:.4f}\")\n",
        "        print(f\"  - Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "        print(f\"  - F1 Score: {metrics['F1 Score']:.4f}\")\n",
        "        print(f\"  - Precision: {metrics['Precision']:.4f}\")\n",
        "        print(f\"  - Recall: {metrics['Recall']:.4f}\")\n",
        "        print(f\"  - Confusion Matrix:\\n{metrics['Confusion Matrix']}\\n\")\n",
        "\n",
        "    print(\"\\nNeural Network Results:\")\n",
        "    print(f\"  - ROC AUC: {nn_results['ROC AUC']:.4f}\")\n",
        "    print(f\"  - Accuracy: {nn_results['Accuracy']:.4f}\")\n",
        "    print(f\"  - F1 Score: {nn_results['F1 Score']:.4f}\")\n",
        "    print(f\"  - Precision: {nn_results['Precision']:.4f}\")\n",
        "    print(f\"  - Recall: {nn_results['Recall']:.4f}\")\n",
        "    print(f\"  - Confusion Matrix:\\n{nn_results['Confusion Matrix']}\\n\")\n",
        "\n",
        "    print(\"\\nEnsemble Model Results:\")\n",
        "    print(f\"  - ROC AUC: {ensemble_results['ROC AUC']:.4f}\")\n",
        "    print(f\"  - Accuracy: {ensemble_results['Accuracy']:.4f}\")\n",
        "    print(f\"  - F1 Score: {ensemble_results['F1 Score']:.4f}\")\n",
        "    print(f\"  - Precision: {ensemble_results['Precision']:.4f}\")\n",
        "    print(f\"  - Recall: {ensemble_results['Recall']:.4f}\")\n",
        "    print(f\"  - Confusion Matrix:\\n{ensemble_results['Confusion Matrix']}\\n\")\n",
        "\n",
        "    # Plot AUC-ROC comparison\n",
        "    plot_auc_roc_curve(base_results, nn_results, ensemble_results, y_val)\n",
        "\n",
        "    # Plot Metrics Comparison\n",
        "    plot_metrics(base_results, nn_results, ensemble_results)\n",
        "\n",
        "    # Plot Confusion Matrices\n",
        "    plot_confusion_matrices(base_results, nn_results, ensemble_results)\n",
        "\n",
        "# Path usage\n",
        "train_df = load_data('Book1.csv')\n",
        "test_df = load_data('data.csv')\n",
        "\n",
        "if train_df is not None and test_df is not None:\n",
        "    run_pipeline(train_df, test_df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1XoFx-GAS1ACt7c9Tjgrj_bDSzs8rdYPe",
      "authorship_tag": "ABX9TyMKQZr01KqzYXN3x80EuzrK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}