{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1CCdAxf6z7bMWRXlz2nJ-kVobr8k1SZnQ",
      "authorship_tag": "ABX9TyOtV5A0oSNFVbIA1ibypEf3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siva1202/Cgpa_calculator/blob/master/Anomaly_detection_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUjKEIQJwc_I",
        "outputId": "95241e0d-a8d6-4038-ed4d-b3328edfb167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Preprocessing Completed!\n",
            "ðŸ“‚ Files Saved:\n",
            "- preprocessed_train_dataset.csv\n",
            "- label_encoders.pkl\n",
            "- scaler.pkl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = \"final_tamilnadu_train_dataset.csv\"  # Change this if your file name is different\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Encode categorical features\n",
        "categorical_features = [\"Payment_Method\", \"Device_Used\", \"Delivery_Location\"]\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Save label encoders\n",
        "with open(\"label_encoders.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "# Step 3: Scale numerical features\n",
        "scaler = MinMaxScaler()\n",
        "df[\"Transaction_Amount\"] = scaler.fit_transform(df[[\"Transaction_Amount\"]])\n",
        "\n",
        "# Save the scaler\n",
        "with open(\"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# Step 4: Save the preprocessed dataset\n",
        "df.to_csv(\"preprocessed_train_dataset.csv\", index=False)\n",
        "\n",
        "# Print confirmation\n",
        "print(\"âœ… Preprocessing Completed!\")\n",
        "print(\"ðŸ“‚ Files Saved:\")\n",
        "print(\"- preprocessed_train_dataset.csv\")\n",
        "print(\"- label_encoders.pkl\")\n",
        "print(\"- scaler.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Step 1: Load preprocessed dataset from Step 1\n",
        "file_path = \"preprocessed_train_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Encode User_ID (Only User_ID needs encoding, others are already done)\n",
        "user_id_encoder = LabelEncoder()\n",
        "df[\"User_ID\"] = user_id_encoder.fit_transform(df[\"User_ID\"])\n",
        "\n",
        "# Save User_ID Encoder\n",
        "with open(\"user_id_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(user_id_encoder, f)\n",
        "\n",
        "# Step 3: Feature Engineering - Compute Aggregates\n",
        "feature_stats = {}\n",
        "\n",
        "# Compute user-based statistics (Saved for later use)\n",
        "feature_stats[\"avg_transaction_amount\"] = df.groupby(\"User_ID\")[\"Transaction_Amount\"].mean()\n",
        "feature_stats[\"transaction_frequency\"] = df.groupby(\"User_ID\")[\"User_ID\"].count()\n",
        "\n",
        "# Apply feature engineering\n",
        "df[\"Avg_Transaction_Amount\"] = df[\"User_ID\"].map(feature_stats[\"avg_transaction_amount\"])\n",
        "df[\"Transaction_Frequency\"] = df[\"User_ID\"].map(feature_stats[\"transaction_frequency\"])\n",
        "df[\"Transaction_Deviation\"] = abs(df[\"Transaction_Amount\"] - df[\"Avg_Transaction_Amount\"])\n",
        "\n",
        "# Detect Unusual Behavior\n",
        "df[\"Most_Frequent_Payment_Method\"] = df.groupby(\"User_ID\")[\"Payment_Method\"].transform(lambda x: x.mode()[0])\n",
        "df[\"Unusual_Payment_Method\"] = (df[\"Payment_Method\"] != df[\"Most_Frequent_Payment_Method\"]).astype(int)\n",
        "\n",
        "df[\"Most_Frequent_Device\"] = df.groupby(\"User_ID\")[\"Device_Used\"].transform(lambda x: x.mode()[0])\n",
        "df[\"Unusual_Device\"] = (df[\"Device_Used\"] != df[\"Most_Frequent_Device\"]).astype(int)\n",
        "\n",
        "# Time-Based Features\n",
        "df[\"Time_Since_Last_Transaction\"] = df.groupby(\"User_ID\")[\"Order_Time\"].diff().fillna(0)\n",
        "df[\"Order_Time_Range\"] = df[\"Order_Time\"].apply(lambda x: 1 if x >= 21 or x < 6 else 0)  # Night (1), Day (0)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(columns=[\"Most_Frequent_Payment_Method\", \"Most_Frequent_Device\"], inplace=True)\n",
        "\n",
        "# Save Feature Engineering Statistics for Later Use\n",
        "with open(\"feature_engineering.pkl\", \"wb\") as f:\n",
        "    pickle.dump(feature_stats, f)\n",
        "\n",
        "# Step 4: Compute Risk Score\n",
        "def calculate_risk_score(row):\n",
        "    deviations = 0\n",
        "    anomaly_types = []\n",
        "\n",
        "    if row[\"Transaction_Deviation\"] > 0.5:\n",
        "        deviations += 1\n",
        "        anomaly_types.append(\"High Transaction Deviation\")\n",
        "    if row[\"Unusual_Payment_Method\"] == 1:\n",
        "        deviations += 1\n",
        "        anomaly_types.append(\"Unusual Payment Method\")\n",
        "    if row[\"Unusual_Device\"] == 1:\n",
        "        deviations += 1\n",
        "        anomaly_types.append(\"Unusual Device\")\n",
        "    if row[\"Order_Time_Range\"] == 1:\n",
        "        deviations += 1\n",
        "        anomaly_types.append(\"Night Order Time\")\n",
        "\n",
        "    # Calculate Risk Score\n",
        "    risk_score = deviations * 0.15 + row[\"Transaction_Deviation\"] * 0.3 + row[\"Time_Since_Last_Transaction\"] * 0.2\n",
        "\n",
        "    # Determine Risk Category\n",
        "    if deviations >= 3:\n",
        "        risk_category = \"High Risk\"\n",
        "    elif deviations == 2:\n",
        "        risk_category = \"Medium Risk\"\n",
        "    elif deviations == 1:\n",
        "        risk_category = \"Low Risk\"\n",
        "    else:\n",
        "        risk_category = \"No Risk\"\n",
        "\n",
        "    return pd.Series([risk_score, risk_category, \", \".join(anomaly_types) if anomaly_types else \"None\"])\n",
        "\n",
        "# Apply Risk Calculation to Dataset\n",
        "df[[\"Risk_Score\", \"Risk_Category\", \"Anomaly_Type\"]] = df.apply(calculate_risk_score, axis=1)\n",
        "\n",
        "# Save the Final Enhanced Dataset\n",
        "df.to_csv(\"enhanced_train_dataset_with_risk.csv\", index=False)\n",
        "\n",
        "# Print Confirmation\n",
        "print(\"âœ… Feature Engineering & Risk-Based Anomaly Detection Completed!\")\n",
        "print(\"ðŸ“‚ Files Saved:\")\n",
        "print(\"- enhanced_train_dataset_with_risk.csv\")\n",
        "print(\"- feature_engineering.pkl\")\n",
        "print(\"- user_id_encoder.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_lTKM-jwsJI",
        "outputId": "1559b3eb-a82b-4a19-97cd-47a1dd7d615e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Feature Engineering & Risk-Based Anomaly Detection Completed!\n",
            "ðŸ“‚ Files Saved:\n",
            "- enhanced_train_dataset_with_risk.csv\n",
            "- feature_engineering.pkl\n",
            "- user_id_encoder.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the enhanced dataset with risk scores\n",
        "df = pd.read_csv(\"enhanced_train_dataset_with_risk.csv\")\n",
        "\n",
        "# Ensure all risk categories exist in the dataset\n",
        "expected_risk_categories = [\"No Risk\", \"Low Risk\", \"Medium Risk\", \"High Risk\"]\n",
        "df[\"Risk_Category\"] = df[\"Risk_Category\"].apply(lambda x: x if x in expected_risk_categories else \"No Risk\")\n",
        "\n",
        "# Manually encode Risk_Category based on the correct order\n",
        "risk_mapping = {\n",
        "    \"No Risk\": 0,\n",
        "    \"Low Risk\": 1,\n",
        "    \"Medium Risk\": 2,\n",
        "    \"High Risk\": 3\n",
        "}\n",
        "df[\"Risk_Category\"] = df[\"Risk_Category\"].map(risk_mapping)\n",
        "\n",
        "# Encode Anomaly_Type using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df[\"Anomaly_Type\"] = le.fit_transform(df[\"Anomaly_Type\"])\n",
        "\n",
        "# Save label encoder for Anomaly_Type\n",
        "with open(\"anomaly_type_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "# Save the final dataset with numerical categorical features\n",
        "df.to_csv(\"final_train_dataset.csv\", index=False)\n",
        "\n",
        "# Print confirmation\n",
        "print(\"âœ… Categorical features converted to numerical values!\")\n",
        "print(\"ðŸ“‚ Files Saved:\")\n",
        "print(\"- final_train_dataset.csv\")\n",
        "print(\"- anomaly_type_encoder.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1oQxI4vw2sa",
        "outputId": "aac7c4a2-f1a9-4f43-9888-c04e52638616"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Categorical features converted to numerical values!\n",
            "ðŸ“‚ Files Saved:\n",
            "- final_train_dataset.csv\n",
            "- anomaly_type_encoder.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "file_path = \"final_train_dataset.csv\"  # Change if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 3: Drop unnecessary columns (keep User_ID)\n",
        "features = df.drop(columns=[\"Risk_Score\", \"Risk_Category\", \"Anomaly\", \"Order_Time\",\"Anomaly_Type\"])  # Remove targets and unnecessary columns\n",
        "target_regression = df[\"Risk_Score\"]\n",
        "target_classification = df[\"Risk_Category\"]\n",
        "\n",
        "# Step 4: Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train_reg, y_test_reg = train_test_split(features, target_regression, test_size=0.2, random_state=42)\n",
        "_, _, y_train_cls, y_test_cls = train_test_split(features, target_classification, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train Regression Model (Risk Score Prediction)\n",
        "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "regressor.fit(X_train, y_train_reg)\n",
        "\n",
        "# Step 6: Train Classification Model (Risk Category Prediction)\n",
        "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train, y_train_cls)\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "y_pred_reg = regressor.predict(X_test)\n",
        "y_pred_cls = classifier.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate Regression Model\n",
        "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Step 9: Evaluate Classification Model\n",
        "accuracy = accuracy_score(y_test_cls, y_pred_cls)\n",
        "classification_rep = classification_report(y_test_cls, y_pred_cls)\n",
        "\n",
        "# Step 10: Save Models\n",
        "with open(\"risk_score_regressor.pkl\", \"wb\") as f:\n",
        "    pickle.dump(regressor, f)\n",
        "\n",
        "with open(\"risk_category_classifier.pkl\", \"wb\") as f:\n",
        "    pickle.dump(classifier, f)\n",
        "\n",
        "# Step 11: Display Results\n",
        "print(\"âœ… Model Training Completed!\")\n",
        "print(\"\\nðŸ“Š Regression Model (Risk Score Prediction):\")\n",
        "print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ² Score: {r2:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Model (Risk Category Prediction):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "print(\"\\nðŸ“‚ Models Saved:\")\n",
        "print(\"- risk_score_regressor.pkl\")\n",
        "print(\"- risk_category_classifier.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_fAxoUpw71I",
        "outputId": "70db9b4c-27fc-47b5-fafe-bab82fe7ee51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model Training Completed!\n",
            "\n",
            "ðŸ“Š Regression Model (Risk Score Prediction):\n",
            "MAE: 0.0032, RMSE: 0.0142, RÂ² Score: 0.9999\n",
            "\n",
            "ðŸ“Š Classification Model (Risk Category Prediction):\n",
            "Accuracy: 1.0000\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2938\n",
            "           1       1.00      1.00      1.00      3416\n",
            "           2       1.00      1.00      1.00      1430\n",
            "           3       1.00      1.00      1.00       218\n",
            "\n",
            "    accuracy                           1.00      8002\n",
            "   macro avg       1.00      1.00      1.00      8002\n",
            "weighted avg       1.00      1.00      1.00      8002\n",
            "\n",
            "\n",
            "ðŸ“‚ Models Saved:\n",
            "- risk_score_regressor.pkl\n",
            "- risk_category_classifier.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"final_train_dataset.csv\")\n",
        "\n",
        "# Define features and targets\n",
        "X = df.drop(columns=[\"Risk_Score\", \"Risk_Category\", \"Anomaly_Type\",\"Anomaly\", \"Order_Time\"])  # Features\n",
        "y_reg = df[\"Risk_Score\"]  # Regression target\n",
        "y_cls = df[\"Risk_Category\"]  # Classification target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_reg_train, y_reg_test, y_cls_train, y_cls_test = train_test_split(\n",
        "    X, y_reg, y_cls, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train XGBoost Regression Model\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_reg.fit(X_train, y_reg_train)\n",
        "\n",
        "# Train XGBoost Classification Model\n",
        "xgb_cls = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_cls.fit(X_train, y_cls_train)\n",
        "\n",
        "# Evaluate\n",
        "reg_preds = xgb_reg.predict(X_test)\n",
        "cls_preds = xgb_cls.predict(X_test)\n",
        "\n",
        "print(\"XGBoost Regression MAE:\", mean_absolute_error(y_reg_test, reg_preds))\n",
        "print(\"XGBoost Classification Accuracy:\", accuracy_score(y_cls_test, cls_preds))\n",
        "\n",
        "# Save models\n",
        "pickle.dump(xgb_reg, open(\"xgb_reg_model.pkl\", \"wb\"))\n",
        "pickle.dump(xgb_cls, open(\"xgb_cls_model.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxTGYh7ew_3K",
        "outputId": "81fa3ecc-f59d-40c1-838b-67254473b846"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Regression MAE: 0.004663468593855192\n",
            "XGBoost Classification Accuracy: 0.9976255936015996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop unnecessary columns before training\n",
        "X_train_filtered = X_train.drop(columns=[\"Anomaly\", \"Order_Time\"], errors=\"ignore\")\n",
        "X_test_filtered = X_test.drop(columns=[\"Anomaly\", \"Order_Time\"], errors=\"ignore\")\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save the scaler\n",
        "pickle.dump(scaler, open(\"mlp_scaler.pkl\", \"wb\"))\n",
        "\n",
        "# Build Regression Model (Risk Score)\n",
        "mlp_reg = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "mlp_reg.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "mlp_reg.fit(X_train_scaled, y_reg_train, epochs=35, batch_size=32, verbose=1)\n",
        "\n",
        "# Save model\n",
        "mlp_reg.save(\"mlp_reg_model.keras\")\n",
        "\n",
        "# Build Classification Model (Risk Category)\n",
        "mlp_cls = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(4, activation=\"softmax\")  # Output layer for 4 classes\n",
        "])\n",
        "mlp_cls.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "mlp_cls.fit(X_train_scaled, y_cls_train, epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# Save model\n",
        "mlp_cls.save(\"mlp_cls_model.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBOPGcz2xDyq",
        "outputId": "a92a9e92-d4ab-4ced-ff18-50f04dc108b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.4101\n",
            "Epoch 2/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0077\n",
            "Epoch 3/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0035\n",
            "Epoch 4/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0020\n",
            "Epoch 5/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0014\n",
            "Epoch 6/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0010\n",
            "Epoch 7/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.3977e-04\n",
            "Epoch 8/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.7678e-04\n",
            "Epoch 9/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.4457e-04\n",
            "Epoch 10/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.0924e-04\n",
            "Epoch 11/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 5.3287e-04\n",
            "Epoch 12/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4.4258e-04\n",
            "Epoch 13/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4.1355e-04\n",
            "Epoch 14/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.8249e-04\n",
            "Epoch 15/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 3.7187e-04\n",
            "Epoch 16/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.8913e-04\n",
            "Epoch 17/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.2676e-04\n",
            "Epoch 18/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.2788e-04\n",
            "Epoch 19/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.1304e-04\n",
            "Epoch 20/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.0312e-04\n",
            "Epoch 21/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.8437e-04\n",
            "Epoch 22/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.4857e-04\n",
            "Epoch 23/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.4824e-04\n",
            "Epoch 24/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2043e-04\n",
            "Epoch 25/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.7886e-04\n",
            "Epoch 26/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.9807e-04\n",
            "Epoch 27/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.6507e-04\n",
            "Epoch 28/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.6005e-04\n",
            "Epoch 29/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.0299e-04\n",
            "Epoch 30/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.4217e-04\n",
            "Epoch 31/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5083e-04\n",
            "Epoch 32/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.4551e-04\n",
            "Epoch 33/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.3981e-04\n",
            "Epoch 34/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.9276e-04\n",
            "Epoch 35/35\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.2674e-04\n",
            "Epoch 1/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8283 - loss: 0.4771\n",
            "Epoch 2/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.0309\n",
            "Epoch 3/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0194\n",
            "Epoch 4/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9946 - loss: 0.0146\n",
            "Epoch 5/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0128\n",
            "Epoch 6/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9962 - loss: 0.0104\n",
            "Epoch 7/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0098\n",
            "Epoch 8/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0093\n",
            "Epoch 9/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0086\n",
            "Epoch 10/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9966 - loss: 0.0080\n",
            "Epoch 11/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0081\n",
            "Epoch 12/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0076\n",
            "Epoch 13/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0074\n",
            "Epoch 14/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9975 - loss: 0.0067\n",
            "Epoch 15/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9977 - loss: 0.0058\n",
            "Epoch 16/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0064\n",
            "Epoch 17/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0056\n",
            "Epoch 18/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0054\n",
            "Epoch 19/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9973 - loss: 0.0070\n",
            "Epoch 20/20\n",
            "\u001b[1m1001/1001\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Load new dataset\n",
        "new_df = pd.read_csv(\"final_tamilnadu_prediction_dataset.csv\")  # Change filename if needed\n",
        "\n",
        "# Load encoders & scaler\n",
        "with open(\"label_encoders.pkl\", \"rb\") as f:\n",
        "    label_encoders = pickle.load(f)\n",
        "\n",
        "with open(\"scaler.pkl\", \"rb\") as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# Apply label encoding on categorical features\n",
        "categorical_features = [\"Payment_Method\", \"Device_Used\", \"Delivery_Location\"]\n",
        "for col in categorical_features:\n",
        "    if col in new_df.columns:\n",
        "        new_df[col] = label_encoders[col].transform(new_df[col])\n",
        "\n",
        "# Scale numerical features\n",
        "if \"Transaction_Amount\" in new_df.columns:\n",
        "    new_df[\"Transaction_Amount\"] = scaler.transform(new_df[[\"Transaction_Amount\"]])\n",
        "\n",
        "# Save preprocessed dataset\n",
        "new_df.to_csv(\"preprocessed_test_dataset.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Step 1: Preprocessing Completed! Saved as 'preprocessed_test_dataset.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUBHIH_SxKtM",
        "outputId": "7e0abd0f-7ab8-424d-b4d8-656322e80f4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Step 1: Preprocessing Completed! Saved as 'preprocessed_test_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load preprocessed dataset\n",
        "new_df = pd.read_csv(\"preprocessed_test_dataset.csv\")\n",
        "\n",
        "# Load user_id encoder and feature stats\n",
        "with open(\"user_id_encoder.pkl\", \"rb\") as f:\n",
        "    user_id_encoder = pickle.load(f)\n",
        "\n",
        "with open(\"feature_engineering.pkl\", \"rb\") as f:\n",
        "    feature_stats = pickle.load(f)\n",
        "\n",
        "# Encode User_ID\n",
        "new_df[\"User_ID\"] = user_id_encoder.transform(new_df[\"User_ID\"])\n",
        "\n",
        "# Compute feature statistics\n",
        "new_df[\"Avg_Transaction_Amount\"] = new_df[\"User_ID\"].map(feature_stats[\"avg_transaction_amount\"])\n",
        "new_df[\"Transaction_Frequency\"] = new_df[\"User_ID\"].map(feature_stats[\"transaction_frequency\"])\n",
        "new_df[\"Transaction_Deviation\"] = abs(new_df[\"Transaction_Amount\"] - new_df[\"Avg_Transaction_Amount\"])\n",
        "\n",
        "# Detect unusual behaviors\n",
        "new_df[\"Most_Frequent_Payment_Method\"] = new_df.groupby(\"User_ID\")[\"Payment_Method\"].transform(lambda x: x.mode()[0])\n",
        "new_df[\"Unusual_Payment_Method\"] = (new_df[\"Payment_Method\"] != new_df[\"Most_Frequent_Payment_Method\"]).astype(int)\n",
        "\n",
        "new_df[\"Most_Frequent_Device\"] = new_df.groupby(\"User_ID\")[\"Device_Used\"].transform(lambda x: x.mode()[0])\n",
        "new_df[\"Unusual_Device\"] = (new_df[\"Device_Used\"] != new_df[\"Most_Frequent_Device\"]).astype(int)\n",
        "\n",
        "# Time-Based Features\n",
        "new_df[\"Time_Since_Last_Transaction\"] = new_df.groupby(\"User_ID\")[\"Order_Time\"].diff().fillna(0)\n",
        "new_df[\"Order_Time_Range\"] = new_df[\"Order_Time\"].apply(lambda x: 1 if x >= 21 or x < 6 else 0)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "new_df.drop(columns=[\"Most_Frequent_Payment_Method\", \"Most_Frequent_Device\"], inplace=True)\n",
        "\n",
        "# Save enhanced dataset\n",
        "new_df.to_csv(\"enhanced_test_dataset.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Step 2: Feature Engineering Completed! Saved as 'enhanced_test_dataset.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sDMIcP9xPAf",
        "outputId": "d7fcea11-9308-4442-9d6c-bc71b4323f4c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Step 2: Feature Engineering Completed! Saved as 'enhanced_test_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Load the feature-engineered test dataset\n",
        "test_df = pd.read_csv(\"enhanced_test_dataset.csv\")\n",
        "\n",
        "# Load models\n",
        "with open(\"risk_score_regressor.pkl\", \"rb\") as f:\n",
        "    rf_regressor = pickle.load(f)\n",
        "\n",
        "with open(\"risk_category_classifier.pkl\", \"rb\") as f:\n",
        "    rf_classifier = pickle.load(f)\n",
        "\n",
        "with open(\"xgb_reg_model.pkl\", \"rb\") as f:\n",
        "    xgb_regressor = pickle.load(f)\n",
        "\n",
        "with open(\"xgb_cls_model.pkl\", \"rb\") as f:\n",
        "    xgb_classifier = pickle.load(f)\n",
        "\n",
        "mlp_reg = tf.keras.models.load_model(\"mlp_reg_model.keras\")\n",
        "mlp_cls = tf.keras.models.load_model(\"mlp_cls_model.keras\")\n",
        "\n",
        "# Load scaler for MLP\n",
        "with open(\"mlp_scaler.pkl\", \"rb\") as f:\n",
        "    mlp_scaler = pickle.load(f)\n",
        "\n",
        "# Load LabelEncoders from a single file\n",
        "with open(\"label_encoders.pkl\", \"rb\") as f:\n",
        "    label_encoders = pickle.load(f)\n",
        "\n",
        "# Select features for prediction (drop unnecessary columns)\n",
        "features = test_df.drop(columns=[\"Order_Time\", \"Anomaly\"], errors=\"ignore\")\n",
        "\n",
        "# Apply models\n",
        "rf_reg_pred = rf_regressor.predict(features)\n",
        "rf_cls_pred = rf_classifier.predict(features)\n",
        "\n",
        "xgb_reg_pred = xgb_regressor.predict(features)\n",
        "xgb_cls_pred = xgb_classifier.predict(features)\n",
        "\n",
        "# Normalize test data for MLP\n",
        "test_scaled = mlp_scaler.transform(features)\n",
        "\n",
        "mlp_reg_pred = mlp_reg.predict(test_scaled).flatten()\n",
        "mlp_cls_pred = mlp_cls.predict(test_scaled).argmax(axis=1)\n",
        "\n",
        "# Ensemble Model (Final Risk Score = Average of Predictions)\n",
        "test_df[\"Risk_Score_Final\"] = (rf_reg_pred + xgb_reg_pred + mlp_reg_pred) / 3\n",
        "\n",
        "# Ensemble Model (Final Risk Category = Majority Vote)\n",
        "test_df[\"Risk_Category_Final\"] = mode(\n",
        "    np.column_stack([rf_cls_pred, xgb_cls_pred, mlp_cls_pred]), axis=1\n",
        ")[0].flatten()\n",
        "\n",
        "# Map numeric Risk Category to labels\n",
        "risk_category_map = {0: \"normal\", 1: \"low risk\", 2: \"medium risk\", 3: \"high risk\"}\n",
        "test_df[\"Risk_Category_Final\"] = test_df[\"Risk_Category_Final\"].map(risk_category_map)\n",
        "\n",
        "# Drop individual model predictions and unnecessary columns\n",
        "columns_to_remove = [\n",
        "    \"Risk_Score_RF\", \"Risk_Score_XGB\", \"Risk_Score_MLP\",\n",
        "    \"Risk_Category_RF\", \"Risk_Category_XGB\", \"Risk_Category_MLP\",\n",
        "    \"Avg_Transaction_Amount\", \"Transaction_Frequency\", \"Transaction_Deviation\",\n",
        "    \"Unusual_Payment_Method\", \"Unusual_Device\", \"Time_Since_Last_Transaction\", \"Order_Time_Range\"\n",
        "]\n",
        "test_df = test_df.drop(columns=[col for col in columns_to_remove if col in test_df.columns])\n",
        "\n",
        "# Reverse the encoding for categorical columns\n",
        "def reverse_encoding(df, encoders):\n",
        "    for column, encoder in encoders.items():\n",
        "        if column in df.columns:\n",
        "            df[column] = encoder.inverse_transform(df[column])\n",
        "    return df\n",
        "\n",
        "# Reverse the encoding for all categorical columns\n",
        "test_df = reverse_encoding(test_df, label_encoders)\n",
        "\n",
        "# Save the final dataset with original categorical values restored\n",
        "test_df.to_csv(\"final_predictions_with_original_values.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Step 4: Predictions Converted to Original Values and Saved as 'final_predictions_with_original_values.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDoSCdLMxU58",
        "outputId": "80798c1f-cb4f-46be-b057-920343f72073"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1251/1251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "\u001b[1m1251/1251\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "âœ… Step 4: Predictions Converted to Original Values and Saved as 'final_predictions_with_original_values.csv'\n"
          ]
        }
      ]
    }
  ]
}